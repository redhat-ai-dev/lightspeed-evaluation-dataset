# Orchestrator in Red Hat Developer Hub

# About Orchestrator in Red Hat Developer Hub

You can streamline and automate your work by using the Orchestrator in Red Hat Developer Hub. It enables you to:

* Design, run, and monitor workflows to simplify multi-step processes across applications and services.
* Standardize onboarding, migration, and integration workflows to reduce manual effort and improve consistency.
* Extend RHDH with enterprise-grade Orchestration features to support collaboration and scalability.


[NOTE]
----
Orchestrator currently supports only Red Hat OpenShift Container Platform (OpenShift Container Platform); it is not available on Microsoft Azure Kubernetes Service (AKS), Amazon Elastic Kubernetes Service (EKS), or Google Kubernetes Engine (GKE).
----

To start using Orchestrator in RHDH, you must:

* Install the required infrastructure components, such as Red Hat OpenShift Serverless Operator, Knative Serving, Knative Eventing, and OpenShift Serverless Logic Operator
* Configure your Backstage custom resource (CR) or Helm values file for Orchestrator

## Understand Orchestrator architecture

The Orchestrator architecture is composed of several components, each contributing to the running and management of workflows.

Red Hat Developer Hub (RHDH):: Serves as the primary interface. It contains the following subcomponents:
Orchestrator frontend plugins:: Provide the interface for users to run and monitor workflows within RHDH.
Orchestrator backend plugins:: Get workflow data into Developer Hub.
Notifications plugins:: Inform users about workflow events.
Sonataflow:: The Sonataflow orchestrator and its subcomponents handle the workflows.
The Red Hat Developer Hub Orchestrator and the Red Hat Developer Hub Helm chart manage the following subcomponents lifecycle:
OpenShift Serverless Logic Operator:: Manages the Sonataflow custom resource (CR), where each CR represents a deployed workflow.
Sonataflow Runtime/Workflow Application:: Functions as a deployed workflow. Operates as an HTTP server, handling requests for running workflow instances. It is managed as a Kubernetes (K8s) deployment by the Openshift Serverless Logic Operator.
Data Index Service:: Serves as a repository for workflow definitions, instances, and associated jobs. It exposes a GraphQL API used by the Orchestrator backend plugin to retrieve workflow definitions and instances.
Job Service:: Orchestrates scheduled tasks for workflows.
OpenShift Serverless:: Provides serverless capabilities essential for workflow communication. It employs Knative eventing to interface with the Data Index service and uses Knative functions to introduce more complex logic to workflows.
PostgreSQL Server:: Provides a database solution essential for data persistence within the Orchestrator ecosystem. The system uses PostgreSQL Server for storing both Sonataflow information and Developer Hub data.
OpenShift AMQ Streams (Strimzi/Kafka):: Provides enhanced reliability of the eventing system. Eventing can work without Kafka by using direct HTTP calls, however, this approach is not reliable.

Optional: The current deployment iteration does not natively integrate or include the AMQ Streams Operator. However, you can add the Operator post-install for enhanced reliability if you require it.

## Compatibility guide for Orchestrator

The following table lists the RHDH Orchestrator plugin versions and their compatible corresponding infrastructure versions.




[NOTE]
----
Orchestrator plugin supports the same OpenShift Container Platform versions as RHDH. See the Life Cycle page.
----

## Orchestrator plugin dependencies for Operator installation

When you enable the Orchestrator plugin in your Backstage custom resource (CR), the Operator automatically provisions the following required dependencies:

* A SonataflowPlatform CR
* NetworkPolicies that allow traffic between infrastructure resources (Knative, Serverless Logic Operator), monitoring traffic, and intra-namespace traffic

The Orchestrator plugin requires these components to run. For example, to communicate with the SonataFlow platform, the Orchestrator plugin uses the sonataflow-platform-data-index-service, which is created by the SonataFlowPlatform CR.


[IMPORTANT]
----
The SonataFlowPlatform CR contains Data Index service that requires PostgreSQL database as shown in the following example:

```yaml
      persistence:
        postgresql:
          secretRef:
            name: backstage-psql-secret-{{backstage-name}}
            userKey: POSTGRES_USER
            passwordKey: POSTGRES_PASSWORD
          serviceRef:
            name: backstage-psql-{{backstage-name}} # # Namespace where the Backstage CR is created
            namespace: {{backstage-ns}} # Namespace where the Backstage (CR) is created
            databaseName: backstage_plugin_orchestrator
```

----

By default, the Orchestrator plugin dependencies use the following:

* The PostgreSQL database named backstage_plugin_orchestrator created by Backstage
* A Secret created by Backstage Operator for the PostgreSQL with POSTGRES_USER and POSTGRES_PASSWORD keys as the database credentials in the Backstage CR namespace.
* A Service created by Backstage Operator for the PostgreSQL database with the name backstage-psql-{{backstage-name}} in the Backstage CR namespace.


[NOTE]
----
To enable the Backstage Operator to work with the SonataFlow platform, its ServiceAccount must have the appropriate permissions.
The Operator automatically creates the required Role and RoleBinding resource in profile/rhdh/plugin-rbac directory.
----

* Dynamic plugins dependency management

## Enabling Orchestrator plugins components

To use the Orchestrator, enable the Orchestrator plugins for Red Hat Developer Hub, that are disabled by default:

Orchestrator frontend plugins:: 
backstage-plugin-orchestrator:: Provides the interface for users to run and monitor workflows within RHDH. You can run and track the execution status of processes.
backstage-plugin-orchestrator-form-widgets:: Provides custom widgets for the workflow execution form, allowing you to customize input fields and streamline the process of launching workflows.
backstage-plugin-orchestrator-form:: Provides the workflow execution form where you can define and submit the necessary input data required to start a new workflow instance.
backstage-plugin-orchestrator-form-api:: Defines the API for extending the workflow execution form.
Orchestrator backend plugins:: 
backstage-plugin-orchestrator-backend:: Gets workflow data into Developer Hub making sure RHDH ingests critical workflow metadata and runtime status fulfilling your need for visibility.
backstage-plugin-orchestrator-common:: Contains the backend OpenAPI specification along with autogenerated API documentation and client libraries.
scaffolder-backend-module-orchestrator:: Provides callable actions from scaffolder templates, such as orchestrator:workflow:run or orchestrator:workflow:get_params.
Notification plugins:: 
backstage-plugin-notifications:: Provides notification frontend components that allow you to display immediate, visible alerts about key workflow state changes, allowing real-time status tracking.
backstage-plugin-signals:: Provides notification frontend components user experience enhancements so you can process the real-time lifecycle events.
backstage-plugin-notifications-backend-dynamic:: Provides notification backend components allowing you to manage and store the stream of workflow events, making sure that critical notifications are ready to be served to the front-end user interface.
backstage-plugin-signals-backend-dynamic:: Provides the backend components for notification user experience enhancements allowing you to establish the necessary communication channels for the event-driven orchestration that is core to Serverless Workflows.

* You have installed the following operators:
* Openshift Serverless
* Openshift Serverless Logic (OSL)
* (Optional) For managing the Orchestrator project, you have an instance of Argo CD or Red Hat OpenShift GitOps in the cluster. It is disabled by default.
* (Optional) To use Tekton tasks and the build pipeline, you have an instance of Tekton or Red Hat OpenShift Pipelines in the cluster. These features are disabled by default.

* Locate your Developer Hub configuration and enable the Orchestrator plugins and the supporting notification plugins.

```yaml
plugins:
  - package: "@redhat/backstage-plugin-orchestrator@1.8.0"
    disabled: false
  - package: "@redhat/backstage-plugin-orchestrator-backend-dynamic@1.8.0"
    disabled: false
  - package: "@redhat/backstage-plugin-scaffolder-backend-module-orchestrator-dynamic@1.8.0"
    disabled: false
  - package: "@redhat/backstage-plugin-orchestrator-form-widgets@1.8.0"
    disabled: false
  - package: "@redhat/backstage-plugin-orchestrator-common@1.8.0"
    disabled: false
  - package: "@redhat/backstage-plugin-orchestrator-form@1.8.0"
    disabled: false
  - package: "@redhat/backstage-plugin-orchestrator-form-api@1.8.0"
    disabled: false
  - package: "./dynamic-plugins/dist/backstage-plugin-notifications"
    disabled: false
  - package: "./dynamic-plugins/dist/backstage-plugin-signals"
    disabled: false
  - package: "./dynamic-plugins/dist/backstage-plugin-notifications-backend-dynamic"
    disabled: false
  - package: "./dynamic-plugins/dist/backstage-plugin-signals-backend-dynamic"
    disabled: false
```


### Installing components using the Orchestrator Infrastructure for Red Hat Developer Hub Helm chart

You can use Orchestrator Infrastructure for Red Hat Developer Hub to install components for the Orchestrator plugins.

1. Run the helm install command for the orchestrator-infra chart. This command initiates the installation of the Red Hat Serverless Operator and Red Hat Serverless Logic Operator components.

```terminal
helm install <release_name> redhat-developer/redhat-developer-hub-orchestrator-infra
```


[NOTE]
----
You must complete this one-off requirement before enabling the Orchestrator plugin.
----
2. Manually approve the install plans for the Operators. You must run the oc patch installplan commands provided in the output to approve their installation.


[IMPORTANT]
----
By default, Orchestrator Infrastructure for Red Hat Developer Hub Helm chart does not auto-approve the required Serverless Operators. You must manually approve the install plans.
----

### Installing Orchestrator components manually on OpenShift Container Platform

Use manual installation when you want full control of the setup process and component versions. Manual installation method focuses on setting up the underlying infrastructure.

1. Install the OpenShift Serverless components manually by following the instructions in the Red Hat OpenShift Serverless documentation.
2. (Optional) If required, deploy a custom PostgreSQL database.


[IMPORTANT]
----
Prevent workflow context from being lost when the Pod restarts by configuring workflow persistence.
You can configure persistence at the namespace level by using the SonataFlowPlatform or SonataFlow custom resources (CR).
For more information, check the Managing workflow persistence documentation.
----

### Installing components using the RHDH helper script

You can use the RHDH helper script plugin-infra.sh to quickly install the OpenShift Serverless infrastructure and Openshift Serverless Logic infrastructure required by the Orchestrator plugin.


[WARNING]
----
Do not use plugin-infra.sh in production.
----

1. Download the plugin-infra.sh script as shown in the following example:

```terminal
curl -sSLO https://raw.githubusercontent.com/redhat-developer/rhdh-operator/refs/heads/release-${PRODUCT_VERSION}/config/profile/rhdh/plugin-infra/plugin-infra.sh # Specify the Red Hat Developer Hub version in the URL or use main
```

2. Run the script:

```terminal
$ ./plugin-infra.sh
```


## Best practices when creating serverless workflows

Create effective serverless workflows using thoughtful approaches to design, handle data, and manage error by following these best practices based on the Serverless Workflow Domain Specific Language (DSL) principles. These principles help you to build robust workflows.

Workflow design principles:: 

The Serverless Workflow DSL prioritizes clarity and ease of use when writing workflows.
Priority of constituencies:: 

When developing workflows or APIs, ensure the needs of the author (workflow writer) come first. The constituencies are prioritized in the following order: Authors &gt; Operators &gt; Implementors &gt; Specifications writers.
Linguistic fluency and clarity:: 
* Use imperative verbs such as Call, Emit, For, Fork, Raise, Run, Set, Switch, and Wait. These simple, universally understood terms make your workflow simple to read and understand.
Structure and extensibility:: 
* Use implicit default behaviors to reduce redundancy.
* Declare components inline if they are not reusable to keep the definition self-contained.
* Use external references to import and reuse shared components, which promotes a modular design.
* Prioritize flexibility over strict enumerations to ensure extensibility and adaptability across different runtime environments.
Data flow and runtime management:: 

Controlling data flow is critical for efficient workflows. Tasks are the fundamental computing units of a workflow. The Domain Specific Language (DSL) defines several default task types that runtimes must do. These include Do, Listen, Raise, Run, Try, and Wait.
Security and error handling:: 
Secrets:: Use Secrets with caution. Avoid passing them directly in call inputs as this might expose sensitive information.
Fault tolerance and error handling:: Serverless Workflow is designed with resilience in mind to recover from failures.
Orchestrator UI integration best practices:: 

For your workflow results to be effectively displayed in the Orchestrator UI and to facilitate chaining of workflows, you must structure the output data according to the WorkflowResult schema. Additionally, include any error information as part of the workflow output so the UI and subsequent workflows can handle them accordingly.
Workflow output schema:: 
Results placement:: The primary output intended for subsequent processing must be placed under the data.result property.
Schema reference:: Your output schema file (schemas/workflow-output-schema.json) must reference the WorkflowResult schema.
Outputs definition:: Include an outputs section in your workflow definition. This section contains human-readable key/value pairs that the UI will display.

Structure of workflow:

```yaml
id: my-workflow
version: "0.8"
specVersion: "0.8"
name: My Workflow
start: ImmediatelyEnd
dataInputSchema: schemas/basic__main-schema.json
extensions:
  - extensionid: workflow-output-schema
    outputSchema: schemas/workflow-output-schema.json
functions:
  - name: print
    type: custom
    operation: sysout
  - name: successResult
    type: expression
    operation: '{
      "result": {
      "message": "Project " + .projectName + " active",
      "outputs":[]
      }
      }'
start: "successResult"
states:
  - name: successResult
    type: operation
    actions:
      - name: setOutput
        functionRef:
          refName: successResult
    end: true
```


# Build and deploy serverless workflows

To deploy a workflow and make it available in the Orchestrator plugin, follow these main steps:

* Building workflow images
* Generating workflow manifests
* Deploying workflows to a cluster

This process moves the workflow from your local machine to deployment on a cluster.

## Benefits of workflow images

While the OpenShift Serverless Logic Operator supports the building of workflows dynamically, this approach is primarily for experimentation.
For production deployments, building images is the preferred method due to the following reasons:

* Production readiness: Prebuilt images can be scanned, secured, and tested before going live.
* GitOps compatibility: The Orchestrator relies on a central OpenShift Serverless Logic Operator instance to track workflows and their state. To use this tracking service, you must deploy workflows with the gitops profile, which expects a prebuilt image.
* Testing and quality: Building an image gives you more control over the testing process.

### Project structure overview

The project utilizes Quarkus project layout (Maven project structure). This structure is illustrated by the following 01_basic workflow example:


```yaml
01_basic
├── pom.xml
├── README.md
└── src
    └── main
        ├── docker
        │   ├── Dockerfile.jvm
        │   ├── Dockerfile.legacy-jar
        │   ├── Dockerfile.native
        │   └── Dockerfile.native-micro
        └── resources
            ├── application.properties
            ├── basic.svg
            ├── basic.sw.yaml
            ├── schemas
            │   ├── basic__main-schema.json
            │   └── workflow-output-schema.json
            └── secret.properties
```


The main workflow resources are located under the src/main/resources/ directory.

The kn-workflow CLI generated this project structure. You can try generating the structure yourself by following the Getting Started guide. For more information on the Quarkus project, see Creating your first application.

### Creating and running your serverless workflow project locally

The kn-workflow CLI is an essential tool that generates workflow manifests and project structures. To ensure successful development and immediate testing, begin developing a new serverless workflow locally by completing the following steps:

1. Use the kn-workflow CLI to create a new workflow project, which adheres to the Quarkus structure as shown in the following example:

```bash
kn-workflow quarkus create --name <specify project name, for example ,00_new_project>
```

2. Edit the workflow, add schema and specific files, and run it locally from project folder as shown in the following example:

```bash
kn-workflow quarkus run
```

3. Run the workflow locally using the kn-workflow run which pulls the following image:

```yaml
registry.redhat.io/openshift-serverless-1/logic-swf-devmode-rhel8:1.36.0
```

4. For building the workflow image, the kn-workflow CLI pulls the following images:

```yaml
registry.redhat.io/openshift-serverless-1/logic-swf-builder-rhel8:1.36.0-8
registry.access.redhat.com/ubi9/openjdk-17:1.21-2
```


* About OpenShift Serverless Logic
* OpenShift Serverless Logic Tutorial
* Running Red Hat Developer Hub behind a corporate proxy
* Using the Red Hat-hosted Quarkus repository

## Building workflow images locally

You can use the build script (build.sh) to build workflow images. You can run it either locally or inside a container. This section highlights how build workflow images locally.

1. Clone the project as shown in the following example:

```yaml
git clone git@github.com:rhdhorchestrator/orchestrator-demo.git
cd orchestrator-demo
```

2. Check the help menu of the script:

```yaml
./scripts/build.sh --help
```

3. Run the build.sh script, providing the required flags, for instance, the image path (-i), workflow source directory (-w), and manifests output directory (-m).

[IMPORTANT]
----
You must specify the full target image path with a tag as shown in the following example:

```yaml
./scripts/build.sh --image=quay.io/orchestrator/demo-basic:test -w 01_basic/ -m 01_basic/manifests
```

----

### The build-sh script functionality and important flags

The build-sh script does the following tasks in order:

* Generates workflow manifests using the kn-workflow CLI.
* Builds the workflow image using podman or docker.
* Optional: The script pushes the images to an image registry and deploys the workflow using kubectl.

You can review the script configuration options and see available flags and their functions by accessing the help menu:


```bash
./scripts/build.sh [flags]
```


The following flags are essential for running the script:




[TIP]
----
The script also supports builder and runtime image overrides, namespace targeting, and persistence flags.
----

### Environment variables supported by the build script

The build-sh script supports the following environment variables to customize the workflow build process without modifying the script itself:

QUARKUS_EXTENSIONS:: The QUARKUS_EXTENSIONS variable specifies additional Quarkus extensions required by the workflow. This variable takes the format of a comma-separated list of fully qualified extension IDs as shown in the following example:

```yaml
export QUARKUS_EXTENSIONS="io.quarkus:quarkus-smallrye-reactive-messaging-kafka"
```


Add Kafka messaging support or other integrations at build time.
MAVEN_ARGS_APPEND:: The MAVEN_ARGS_APPEND variable appends additional arguments to the Maven build command. This variable takes the format of a string of Maven CLI arguments as shown in the following example:

```yaml
export MAVEN_ARGS_APPEND="-DmaxYamlCodePoints=35000000"
```


Control build behavior. For example, set maxYamlCodePoints parameter that controls the maximum input size for YAML input files to 35000000 characters (~33MB in UTF-8).

### Required tools

To run the build-sh script locally and manage the workflow lifecycle, you must install the following command-line tools:



### Building the 01_basic workflow

To run the script from the root directory of the repository, you must use the -w flag to point to the workflow directory. Additionally, specify the output directory with the -m flag.

* You have specified the target image using a tag.

1. Run the following command:

```bash
./scripts/build.sh --image=quay.io/orchestrator/demo-basic:test -w 01_basic/ -m 01_basic/manifests
```


This build command produces the following two artifacts:
* A workflow image and Kubernetes manifests: quay.io/orchestrator/demo-basic:test and tagged as latest.
* Kubernetes manifests under: 01_basic/manifests/
2. Optional: You can add the --push flag to automatically push the image after building. Otherwise, pushing manually is mandatory before deploying.

## Generated workflow manifests

The following example is an illustration of what is generated under the 01_basic/manifests:


```yaml
01_basic/manifests
├── 00-secret_basic-secrets.yaml
├── 01-configmap_basic-props.yaml
├── 02-configmap_01-basic-resources-schemas.yaml
└── 03-sonataflow_basic.yaml
```


00-secret_basic-secrets.yaml:: Contains secrets from 01_basic/src/main/resources/secret.properties.
Values are not required at this stage as you can set them later after applying CRs or when using GitOps.

In OpenShift Serverless Logic v1.36, after updating a secret, you must manually restart the workflow Pod for changes to apply.

01-configmap_basic-props.yaml:: Holds application properties from application.properties.
Any change to this ConfigMap triggers an automatic Pod restart.
02-configmap_01-basic-resources-schemas.yaml:: Contains JSON schemas from src/main/resources/schemas.

[NOTE]
----
You do not need to deploy certain configuration resources when using the GitOps profile.
----
03-sonataflow_basic.yaml:: The SonataFlow custom resource (CR) that defines the workflow.

```yaml
podTemplate:
  container:
    image: quay.io/orchestrator/demo-basic
    resources: {}
    envFrom:
      - secretRef:
          name: basic-secrets
```


```yaml
persistence:
  postgresql:
    secretRef:
      name: sonataflow-psql-postgresql
      userKey: <your_postgres_username>
      passwordKey: <your_postgres_password>
    serviceRef:
      name: sonataflow-psql-postgresql
      port: 5432
      databaseName: sonataflow
      databaseSchema: basic
```


where:
postgresql:secretRef:name:: Enter the Secret name for your deployment.
postgresql:secretRef:userKey:: Enter the key for your deployment.
postgresql:secretRef:passwordKey:: Enter the password for your deployment.
postgresql:serviceRef:name:: Enter the Service name for your deployment.

If you must connect to an external database, replace serviceRef with jdbcUrl. See Managing workflow persistence.

By default, the script generates all the manifests without a namespace. You can specify a namespace to the script by using the --namespace flag if you know the target namespace in advance. Otherwise, you must provide the namespace when applying the manifests to the cluster. See Configuring workflow services.

## Deploying workflows on a cluster

You can deploy the workflow on a cluster, since the image is pushed to the image registry and the deployment manifests are available.

* You have an OpenShift Container Platform cluster with the following versions of components installed:
* Red Hat Developer Hub (RHDH) v1.7
* Orchestrator plugins v1.7.1
* OpenShift Serverless v1.36
* OpenShift Serverless Logic v1.36

For instructions on how to install these components, see the Orchestrator plugin components on OpenShift Container Platform.
* You must apply the workflow manifests in a namespace that contains a SonataflowPlatform custom resource (CR), which manages the supporting services.

1. Use the kubectl create command specifying the target namespace to apply the Kubernetes manifests as shown in the following example:

```bash
kubectl create -n <your_namespace> -f ./01_basic/manifests/.
```

2. After deployment, monitor the status of the workflow pods as shown in the following example:

```yaml
kubectl get pods -n <your_namespace> -l app=basic
```


The pod may initially appear in an Error state because of missing or incomplete configuration in the Secret or ConfigMap.
3. Inspect the Pod logs as shown in the following example:

```yaml
oc logs -n <your_namespace> basic-f7c6ff455-vwl56
```


The following code is an example of the output:

```yaml
SRCFG00040: The config property quarkus.openapi-generator.notifications.auth.BearerToken.bearer-token is defined as the empty String ("") which the following Converter considered to be null: io.smallrye.config.Converters$BuiltInConverter
java.lang.RuntimeException: Failed to start quarkus
...
Caused by: io.quarkus.runtime.configuration.ConfigurationException: Failed to read configuration properties
```


The error indicates a missing property: quarkus.openapi-generator.notifications.auth.BearerToken.bearer-token.
4. In such a case where the logs show the ConfigurationException: Failed to read configuration properties error or indicate a missing value, retrieve the ConfigMap as shown in the following example:

```yaml
oc get -n <your_namespace> configmaps basic-props -o yaml
```


The following code is an example of the sample output:

```yaml
apiVersion: v1
data:
  application.properties: |
    # Backstage notifications service
    quarkus.rest-client.notifications.url=${BACKSTAGE_NOTIFICATIONS_URL}
    quarkus.openapi-generator.notifications.auth.BearerToken.bearer-token=${NOTIFICATIONS_BEARER_TOKEN}
...
```


Resolve the placeholders using values provided using a Secret.
5. You must edit the corresponding Secret and provide appropriate base64-encoded values to resolve the placeholders in application.properties as shown in the following example:

```yaml
kubectl edit secrets -n <your_namespace> basic-secrets
```

6. Restart the workflow Pod for Secret changes to take effect in OpenShift Serverless Logic v1.36.

1. Verify the deployment status by checking the Pods again as shown in the following example:

```yaml
oc get pods -n <your_namespace> -l app=basic
```


The expected status for a successfully deployed workflow Pod is as shown in the following example:

```yaml
NAME                    READY   STATUS    RESTARTS   AGE
basic-f7c6ff455-grkxd   1/1     Running   0          47s
```

2. Once the Pod is in the Running state, the workflow now appears in the Orchestrator plugin inside the Red Hat Developer Hub.

* Inspect the provided build script to extract the actual steps and implement them in your preferred CI/CD tool, for example, GitHub Actions, GitLab CI, Jenkins, and Tekton.

# Installing Red Hat Developer Hub with Orchestrator

To install Red Hat Developer Hub, use one of the following methods:

* The Red Hat Developer Hub Operator
* The Red Hat Developer Hub Helm chart

## Enabling the Orchestrator plugin using Operator

You can enable the Orchestrator plugin in RHDH by configuring dynamic plugins in your Backstage custom resource (CR).

* You have installed RHDH on OpenShift Container Platform.
* You have access to edit or create ConfigMaps in the namespace where the Backstage CR is deployed.

1. To enable the Orchestrator plugin with default settings, set disabled: false for the package. For example, package: "@redhat/backstage-plugin-orchestrator@<plugin_version> is set to disabled: false:

```yaml
- package: "@redhat/backstage-plugin-orchestrator@<plugin_version>"
  disabled: false
```

Example: Complete configuration of the Orchestrator plugin

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: orchestrator-plugin
data:
    dynamic-plugins.yaml: |
      includes:
        - dynamic-plugins.default.yaml
      plugins:
        - package: "@redhat/backstage-plugin-orchestrator@1.7.1"
          disabled: false
          pluginConfig:
            dynamicPlugins:
                frontend:
                  red-hat-developer-hub.backstage-plugin-orchestrator:
                    appIcons:
                      - importName: OrchestratorIcon
                        name: orchestratorIcon
                    dynamicRoutes:
                      - importName: OrchestratorPage
                        menuItem:
                          icon: orchestratorIcon
                          text: Orchestrator
                        path: /orchestrator
                    entityTabs:
                      - path: /workflows
                        title: Workflows
                        mountPoint: entity.page.workflows
                    mountPoints:
                      - mountPoint: entity.page.workflows/cards
                      importName: OrchestratorCatalogTab
                      config:
                        layout:
                          gridColumn: '1 / -1'
                          if:
                            anyOf:
                              - IsOrchestratorCatalogTabAvailable
        - package: "@redhat/backstage-plugin-orchestrator-backend-dynamic@1.7.1"
          disabled: false
          pluginConfig:
            orchestrator:
              dataIndexService:
                url: http://sonataflow-platform-data-index-service
          dependencies:
            - ref: sonataflow
        - package: "@redhat/backstage-plugin-scaffolder-backend-module-orchestrator-dynamic@1.7.1"
          disabled: false
          pluginConfig:
            orchestrator:
              dataIndexService:
                url: http://sonataflow-platform-data-index-service
        - package: "@redhat/backstage-plugin-orchestrator-form-widgets@1.7.1"
          disabled: false
          pluginConfig:
            dynamicPlugins:
              frontend:
                red-hat-developer-hub.backstage-plugin-orchestrator-form-widgets: { }
---
apiVersion: rhdh.redhat.com/v1alpha3
kind: Backstage
metadata:
  name: orchestrator
spec:
  application:
    appConfig:
      configMaps:
        - name: app-config-rhdh
    dynamicPluginsConfigMapName: orchestrator-plugin
```

2. Create a secret containing the BACKEND_SECRET value as shown in the following example:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config-rhdh
data:
  app-config.yaml: |-
    auth:
      environment: development
      providers:
        guest:
          # using the guest user to query the '/api/dynamic-plugins-info/loaded-plugins' endpoint.
          dangerouslyAllowOutsideDevelopment: true
    backend:
      auth:
        externalAccess:
          - type: static
            options:
              token: ${BACKEND_SECRET}
              subject: orchestrator
---
apiVersion: v1
kind: Secret
metadata:
  name: backend-auth-secret
stringData:
  # generated with the command below (from https://backstage.io/docs/auth/service-to-service-auth/#setup):
  # node -p 'require("crypto").randomBytes(24).toString("base64")'
  # notsecret
  BACKEND_SECRET: "R2FxRVNrcmwzYzhhN3l0V1VRcnQ3L1pLT09WaVhDNUEK"
```

3. Configure your Backstage CR to update the secret name in the extraEnvs field as shown in the following example:

```yaml
apiVersion: rhdh.redhat.com/v1alpha4
kind: Backstage
metadata:
  name: orchestrator
spec:
  application:
    appConfig:
      configMaps:
        - name: app-config-rhdh
    dynamicPluginsConfigMapName: orchestrator-plugin
    extraEnvs:
      secrets:
          # secret that contains the BACKEND_SECRET key
        - name: backend-auth-secret
```


* In the RHDH console, confirm that the Orchestrator frontend and backend features are available.

## Upgrading the Orchestrator plugin from 1.7 to 1.8

You can upgrade an existing 1.7 Operator-backed instance with Orchestrator enabled by upgrading the Red Hat Developer Hub Operator to 1.8. After upgrading the Operator to 1.8, manually update the dynamic-plugins ConfigMap to set the Orchestrator plugin versions to 1.8.2.

* You have have a running instance of Red Hat Developer Hub with Orchestrator 1.7 backed by the Operator.
* You have upgraded the Red Hat Developer Hub Operator to 1.8.

1. Update your dynamic-plugins ConfigMap to set the version of the Orchestrator plugin to 1.8.2.

The following YAML configuration is an example of a dynamic-plugins ConfigMap enabling the Orchestrator plugins in RHDH for Operator-backed instances:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: dynamic-plugins-rhdh
data:
  dynamic-plugins.yaml: |
    includes:
      - dynamic-plugins.default.yaml
    plugins:
      - package: "@redhat/backstage-plugin-orchestrator@1.8.2"
        disabled: false
      - package: "@redhat/backstage-plugin-orchestrator-backend-dynamic@1.8.2"
        disabled: false
        dependencies:
          - ref: sonataflow
      - package: "@redhat/backstage-plugin-scaffolder-backend-module-orchestrator-dynamic@1.8.2"
        disabled: false
      - package: "@redhat/backstage-plugin-orchestrator-form-widgets@1.8.2"
        disabled: false
```


1. Navigate to your Red Hat Developer Hub instance.

[NOTE]
----
The upgrade takes a few minutes to complete. The Red Hat Developer Hub version does not update in the UI until all running Backstage pods are successfully recreated.
----
2. From the profile dropdown in the top menu, click the Settings icon and then locate the RHDH metadata card.
3. Confirm that the value displayed for RHDH version is 1.8.
4. Alternatively, run the following command in your terminal to wait for all pods in the current project to be in the running state:

```terminal
oc get pods -w
```


The upgrade is successful when all Backstage-related pods show a stable Running status.

## Installing Red Hat Developer Hub (RHDH) on OpenShift Container Platform with the Orchestrator using the Helm CLI

You can install Red Hat Developer Hub (RHDH) on OpenShift Container Platform with the Orchestrator by using the Helm CLI. The installation automatically enables the required dynamic plugins and integrates workflow infrastructure.

* You are logged in as an administrator and have access to the Red Hat Developer Hub Helm chart repository.
* You can install the necessary infrastructures resources, such as SonataFlow, alongside RHDH in the same namespace.

This is a one-off requirement and must be completed before enabling the Orchestrator plugin.

1. As an administrator, install relevant cluster-wide resources.

```yaml
helm repo add openshift-helm-charts https://charts.openshift.io/
helm install <release_name> openshift-helm-charts/redhat-developer-hub-orchestrator-infra
```


[IMPORTANT]
----
You must be an administrator to install the redhat-developer-hub-orchestrator-infra Helm chart because it deploys additional cluster-scoped OpenShift Serverless and OpenShift Serverless Logic Operators. As an administrator, you must manually approve the install plans for OpenShift Serverless and Serverless Logic Operators.
----
2. Install the Backstage chart with the orchestrator enabled as shown in the following example:

```terminal
$ helm install <release_name> openshift-helm-charts/redhat-developer-hub --version 1.8.0 \
  --set orchestrator.enabled=true
```

3. (Optional) Enable Notifications and Signals plugins by adding them to the global.dynamic.plugins list in your values.yaml file as shown in the following example:

```yaml
global:
  dynamic:
    plugins:
      - disabled: false
        package: "./dynamic-plugins/dist/backstage-plugin-notifications"
      - disabled: false
        package: "./dynamic-plugins/dist/backstage-plugin-signals"
      - disabled: false
        package: "./dynamic-plugins/dist/backstage-plugin-notifications-backend-dynamic"
      - disabled: false
        package: "./dynamic-plugins/dist/backstage-plugin-signals-backend-dynamic"
```

4. (Optional) You can disable the Serverless Logic and Serverless Operators individually or together by setting their values to false, as shown in the following example:

```terminal
helm install <release_name> openshift-helm-charts/redhat-developer-hub \
  --version 1.8.0 \
  --set orchestrator.enabled=true \
  --set orchestrator.serverlessOperator=false \
  --set orchestrator.serverlessLogicOperator=false
```

5. (Optional) If you are using an external database, add the following configuration under orchestrator.sonataflowPlatform in your values.yaml file:

```yaml
orchestrator:
  sonataflowPlatform:
    externalDBsecretRef: "<cred-secret>"
    externalDBName: "<database_name>" # The name of the user-configured existing database (Not the database that the orchestrator and sonataflow resources use).
    externalDBHost: "<database_host>"
    externalDBPort: "<database_port>"
```


[NOTE]
----
This step only configures the Orchestrators use of an external database. To configure Red Hat Developer Hub to use an external PostgreSQL instance, follow the steps in Configuring a PostgreSQL instance using Helm.
----

1. Verify that the Orchestrator plugin is visible in the Red Hat Developer Hub UI.
2. Create and run sample workflows to confirm the orchestration is functioning correctly.

## Install Red Hat Developer Hub (RHDH) using Helm from the OpenShift Container Platform web console

You can install Red Hat Developer Hub (RHDH) with the Orchestrator by using the (OpenShift Container Platform) web console. This method is useful if you prefer a graphical interface or want to deploy cluster-wide resources without using the Helm CLI.

* You are logged in to the OpenShift Container Platform web console as an administrator.
* You have access to the Red Hat Developer Hub Helm chart repository.
* Your cluster has internet access or the Helm charts are mirrored in a disconnected environment.

1. In the OpenShift Container Platform web console, go to the Helm Charts and verify that the Red Hat Developer Hub Helm chart repository is available.
2. Search for the Orchestrator infrastructure for Red Hat Developer Hub and select Install.

[IMPORTANT]
----
You must be an administrator to install the Orchestrator Infrastructure for Red Hat Developer Hub Helm chart because it deploys cluster-scoped resources. As an administrator, you must manually approve the install plans for OpenShift Serverless and Serverless Logic Operators.
----

As a regular user, search for the Red Hat Developer Hub chart and install it by setting the value of orchestrator.enabled to true. Otherwise, the Orchestrator will not be deployed.
3. Wait until they are successfully deployed.
4. Monitor the deployment status by navigating to Pods or releases.

After deployment completes:

* The orchestrator-related pods are running in the selected namespace.
* Cluster-wide resources are present.
* You can start connecting the orchestrator to your Red Hat Developer Hub UI.

## Resource limits for installing Red Hat Developer Hub with the Orchestrator plugin when using Helm

When installing Red Hat Developer Hub (RHDH) with the Orchestrator plugin using Helm, the chart defines default CPU and memory limits for the SonataFlowPlatform component.

These limits are enforced by the cluster so that pods do not exceed their allocated resources.

1. Default resource limits



1. You can override these values in any of the following ways:
* With values.yaml
* With --set flags
2. Override defaults with values.yaml as shown in the following example:

```yaml
orchestrator:
  enabled: true
  sonataflowPlatform:
  resources:
      limits:
        cpu: "500m"
        memory: "1Gi"
```

3. Override with --set as shown in the following example:

```terminal
helm upgrade --install <release_name>  openshift-helm-charts/redhat-developer-hub \
  --set orchestrator.enabled=true \
  --set orchestrator.sonataflowPlatform.resources.requests.cpu=500m \
  --set orchestrator.sonataflowPlatform.resources.requests.memory=128Mi \
  --set orchestrator.sonataflowPlatform.resources.limits.cpu=1 \
  --set orchestrator.sonataflowPlatform.resources.limits.memory=2Gi
```


[NOTE]
----
The --set setting is applicable only when orchestrator.enabled is true. By default, it is set to false.
----

# Installing Orchestrator plugin in an air-gapped environment with the Helm chart

You can configure Red Hat Developer Hub (RHDH) with the Orchestrator plugin in a fully disconnected or partially disconnected environment by using the Helm chart.

## Installing Red Hat Developer Hub with Orchestrator in a fully disconnected OpenShift Container Platform environment using the Helm chart

You can install Red Hat Developer Hub (RHDH) with the Orchestrator plugin in a fully air-gapped OpenShift Container Platform environment using the Helm chart.

You can mirror images to an intermediary disk, and then mirror from the disk to your target local registry and apply the generated cluster resources.

* You have set up your disconnected environment using a local registry.
* You have permissions to push NPM packages to an NPM server available in your restricted network.
* You have installed the oc-mirror tool, with a version corresponding to the version of your OpenShift Container Platform cluster.

1. Create an ImageSetConfiguration.yaml file for oc-mirror. You must use an ImageSetConfiguration file to include all mirrored images required by the Serverless Logic Operator, as shown in the following example:

```yaml
apiVersion: mirror.openshift.io/v2alpha1
kind: ImageSetConfiguration
mirror:
  additionalimages:
  - name: registry.redhat.io/openshift-serverless-1/logic-jobs-service-postgresql-rhel8:1.36.0
  - name: registry.redhat.io/openshift-serverless-1/logic-jobs-service-ephemeral-rhel8:1.36.0
  - name: registry.redhat.io/openshift-serverless-1/logic-data-index-postgresql-rhel8:1.36.0
  - name: registry.redhat.io/openshift-serverless-1/logic-data-index-ephemeral-rhel8:1.36.0
  - name: registry.redhat.io/openshift-serverless-1/logic-db-migrator-tool-rhel8:1.36.0
  - name: registry.redhat.io/openshift-serverless-1/logic-swf-builder-rhel8:1.36.0
  - name: registry.redhat.io/openshift-serverless-1/logic-swf-devmode-rhel8:1.36.0

  helm:
    repositories:
      - name: openshift-charts
        url: https://charts.openshift.io
        charts:
          - name: redhat-developer-hub
            version: "1.8.0"
          - name: redhat-developer-hub-orchestrator-infra
            version: "1.8.0"
  operators:
    - catalog: registry.redhat.io/redhat/redhat-operator-index:4.19
      # For example: registry.redhat.io/redhat/redhat-operator-index:v4.19
      packages:
      - name: logic-operator-rhel8
        channels:
        - name: alpha
          minVersion: 1.36.0
          maxVersion: 1.36.0
      - name: serverless-operator
        channels:
        - name: stable
          minVersion: 1.36.0
          maxVersion: 1.36.1
```


Alternatively, you can use podman commands to find the missing images and add them to the additionalimages list if the versions change:

```terminal
IMG=registry.redhat.io/openshift-serverless-1/logic-operator-bundle:1.36
mkdir local-manifests-osl
podman create --name temp-container "$IMG" -c "cat /manifests/logic-operator-rhel8.clusterserviceversion.yaml"
podman cp temp-container:/manifests ./local-manifests-osl
podman rm temp-container
yq -r '.data."controllers_cfg.yaml" | from_yaml | .. | select(tag == "!!str") | select(test("^.\\/.:.*$"))' ./local-manifests-osl/manifests/logic-operator-rhel8-controllers-config_v1_configmap.yaml
```

2. Mirror the images in the ImageSetConfiguration.yaml file by running the oc-mirror command. For example:

```terminal
oc-mirror --config=ImageSetConfiguration.yaml file:///path/to/mirror-archive --authfile /path/to/authfile  --v2
```


[NOTE]
----
The oc-mirror command pulls the charts listed in the ImageSetConfiguration file and makes them available as tgz archives under the /path/to/mirror-archive directory.
----
3. Apply the cluster-wide resources generated during the push step to redirect all image pulls to your local registry, as shown in the following example:

```terminal
cd <workspace folder>/working-dir/cluster-resources/
oc apply -f .
```

4. Transfer the generated mirror archive file, for example, /path/to/mirror-archive/mirror_000001.tar, to a bastion host within your disconnected environment.
5. From the bastion host in your disconnected environment, which has access to the mirror registry, mirror the images from the archive file to your target registry. For example:

```terminal
oc-mirror --v2 --from <mirror-archive-file> docker://<target-registry-url:port> --workspace file://<workspace folder> --authfile /path/to/authfile
```


where:
<mirror-archive-file>:: Enter the name of the transferred tar file.
<target-registry-url:port>:: Enter your local registry, for example, registry.localhost:5000.
6. Download the Node Package Manager (NPM) packages for orchestrator 1.8.0 using any of the following methods:
* Download them as tgz files from the following registry:
* https://npm.registry.redhat.com/@redhat/backstage-plugin-orchestrator/-/backstage-plugin-orchestrator-1.8.0.tgz
* https://npm.registry.redhat.com/@redhat/backstage-plugin-orchestrator-backend-dynamic/-/backstage-plugin-orchestrator-backend-dynamic-1.8.0.tgz
* https://npm.registry.redhat.com/@redhat/backstage-plugin-scaffolder-backend-module-orchestrator-dynamic/-/backstage-plugin-scaffolder-backend-module-orchestrator-dynamic-1.8.0.tgz
* https://npm.registry.redhat.com/@redhat/backstage-plugin-orchestrator-form-widgets/-/backstage-plugin-orchestrator-form-widgets-1.8.0.tgz
* Alternatively, use the NPM packages from Red Hat NPM registry as shown in the following example:

```
npm pack "@redhat/backstage-plugin-orchestrator@1.8.0" --registry=https://npm.registry.redhat.com
npm pack "@redhat/backstage-plugin-orchestrator-backend-dynamic@1.8.0" --registry=https://npm.registry.redhat.com
npm pack "@redhat/backstage-plugin-scaffolder-backend-module-orchestrator-dynamic@1.8.0 --registry=https://npm.registry.redhat.com
npm pack "@redhat/backstage-plugin-orchestrator-form-widgets@1.8.0" --registry=https://npm.registry.redhat.com
```

7. Push the NPM packages you have downloaded to your NPM server, as shown in the following example:

```
npm publish backstage-plugin-orchestrator-1.8.0.tgz
npm publish backstage-plugin-orchestrator-backend-dynamic-1.8.0.tgz
npm publish backstage-plugin-orchestrator-form-widgets-1.8.0.tgz
npm publish backstage-plugin-scaffolder-backend-module-orchestrator-dynamic-1.8.0.tgz
```

8. Apply the redhat-developer-hub-orchestrator-infra Helm chart and approve the install plans. See Air-gapped installation with Helm chart instructions for details.
9. Apply the RHDH 1.8 Helm chart. Include the version 1.8.0 and enable the Orchestrator plugin, as shown in the following example:

```yaml
orchestrator.enabled=true
```

10. The RHDH 1.8 Helm chart defaults to pulling Orchestrator plugins from the official Red Hat NPM registry using full URL references. You must override this behavior to point to your local registry.

To configure the Orchestrator plugins to use a custom registry, complete the following steps:
* Open your values.yaml file.
* Explicitly list the Orchestrator plugin packages under the orchestrator.plugins  section.
You must replace the simplified package references with the full URLs that point to your custom NPM registry, as shown in the following example:

```yaml
orchestrator:
  plugins:
    - disabled: false
      package: <custom_NPM_registry_URL>[:<port>]/@redhat/backstage-plugin-orchestrator-backend-dynamic/-/backstage-plugin-orchestrator-backend-dynamic-{product-bundle-version}.tgz
      integrity: sha512-xxxxxx
    - disabled: false
      package: <custom_NPM_registry_URL>[:<port>]/@redhat/backstage-plugin-orchestrator/-/backstage-plugin-orchestrator-{product-bundle-version}.tgz
      integrity: sha512-xxxxxy
    - disabled: false
      package: <custom_NPM_registry_URL>[:<port>]/@redhat/backstage-plugin-orchestrator-form-widgets/-/backstage-plugin-orchestrator-form-widgets-{product-bundle-version}.tgz
      integrity: sha512-xxxxxz
    - disabled: false
      package: <custom_NPM_registry_URL>[:<port>]/@redhat/backstage-plugin-scaffolder-backend-module-orchestrator-dynamic/-/backstage-plugin-scaffolder-backend-module-orchestrator-dynamic-{product-bundle-version}.tgz
      integrity: sha512-xxxx1
```


where:
<custom_NPM_registry_URL>:: Enter the address of your custom registry and make sure the integrity checksum, such as sha512-xxxxxx, matches the files in your registry.

* Restart the RHDH Pod and wait for the components to deploy properly.
* After deployment is complete, go to the RHDH UI and confirm that the Orchestrator UI is accessible and functioning correctly.


[NOTE]
----
The successful accessibility of the Orchestrator UI confirms that the underlying components are running and the cluster recognizes the plugin.
----

## Installing Red Hat Developer Hub with Orchestrator in a partially disconnected OpenShift Container Platform environment using the Helm chart

You can install Red Hat Developer Hub (RHDH) with the Orchestrator plugin in a partial OpenShift Container Platform environment using the Helm chart.

A disconnected installation prevents unauthorized access, data transfer, or communication with external sources.

You can use the oc-mirror command to mirror resources directly to your accessible local registry and apply the generated cluster resources.

* You have set up your disconnected environment using a local registry.
* You have permissions to push NPM packages to an NPM server available in your restricted network.
* You have installed the oc-mirror tool, with a version corresponding to the version of your OpenShift Container Platform cluster.

1. Create an ImageSetConfiguration file for oc-mirror. You must include the images and operators required by the Serverless Logic Operator in the ImageSetConfiguration file, as oc-mirror does not automatically mirror all images. Use the following example:

```yaml
apiVersion: mirror.openshift.io/v2alpha1
kind: ImageSetConfiguration
mirror:
  additionalimages:
  - name: registry.redhat.io/openshift-serverless-1/logic-jobs-service-postgresql-rhel8:1.36.0
  - name: registry.redhat.io/openshift-serverless-1/logic-jobs-service-ephemeral-rhel8:1.36.0
  - name: registry.redhat.io/openshift-serverless-1/logic-data-index-postgresql-rhel8:1.36.0
  - name: registry.redhat.io/openshift-serverless-1/logic-data-index-ephemeral-rhel8:1.36.0
  - name: registry.redhat.io/openshift-serverless-1/logic-db-migrator-tool-rhel8:1.36.0
  - name: registry.redhat.io/openshift-serverless-1/logic-swf-builder-rhel8:1.36.0
  - name: registry.redhat.io/openshift-serverless-1/logic-swf-devmode-rhel8:1.36.0

  helm:
    repositories:
      - name: openshift-charts
        url: https://charts.openshift.io
        charts:
          - name: redhat-developer-hub
            version: "1.8.0"
          - name: redhat-developer-hub-orchestrator-infra
            version: "1.8.0"
  operators:
    - catalog: registry.redhat.io/redhat/redhat-operator-index:4.19
      # For example: registry.redhat.io/redhat/redhat-operator-index:v4.19
      packages:
      - name: logic-operator-rhel8
        channels:
        - name: alpha
          minVersion: 1.36.0
          maxVersion: 1.36.0
      - name: serverless-operator
        channels:
        - name: stable
          minVersion: 1.36.0
          maxVersion: 1.36.1
```

2. Mirror the images in the ImageSetConfiguration.yaml file by running the oc-mirror command to pull images and charts, and push the images directly to the target registry. For example:

```terminal
oc-mirror --config=imagesetconfiguration.yaml docker://<registry URL:port> --workspace file://<workspace folder> --authfile /path/to/authfile  --v2
```


[NOTE]
----
The oc-mirror command pulls the charts listed in the ImageSetConfiguration file and makes them available as tgz archives under the <workspace folder> directory.
----
3. Apply the generated cluster resources to the disconnected cluster. For example:

```terminal
cd <workspace folder>/working-dir/cluster-resources/
oc apply -f .
```

4. Download the Node Package Manager (NPM) packages for orchestrator 1.8.0 using any of the following methods:
* Download them as tgz files from the following registry:
* https://npm.registry.redhat.com/@redhat/backstage-plugin-orchestrator/-/backstage-plugin-orchestrator-1.8.0.tgz
* https://npm.registry.redhat.com/@redhat/backstage-plugin-orchestrator-backend-dynamic/-/backstage-plugin-orchestrator-backend-dynamic-1.8.0.tgz
* https://npm.registry.redhat.com/@redhat/backstage-plugin-scaffolder-backend-module-orchestrator-dynamic/-/backstage-plugin-scaffolder-backend-module-orchestrator-dynamic-1.8.0.tgz
* https://npm.registry.redhat.com/@redhat/backstage-plugin-orchestrator-form-widgets/-/backstage-plugin-orchestrator-form-widgets-1.8.0.tgz
* Alternatively, use the NPM packages from Red Hat NPM registry as shown in the following example:

```
npm pack "@redhat/backstage-plugin-orchestrator@1.8.0" --registry=https://npm.registry.redhat.com
npm pack "@redhat/backstage-plugin-orchestrator-backend-dynamic@1.8.0" --registry=https://npm.registry.redhat.com
npm pack "@redhat/backstage-plugin-scaffolder-backend-module-orchestrator-dynamic@1.8.0 --registry=https://npm.registry.redhat.com
npm pack "@redhat/backstage-plugin-orchestrator-form-widgets@1.8.0" --registry=https://npm.registry.redhat.com
```

5. Push the NPM packages you have downloaded to your NPM server, as shown in the following example:

```
npm publish backstage-plugin-orchestrator-1.8.0.tgz
npm publish backstage-plugin-orchestrator-backend-dynamic-1.8.0.tgz
npm publish backstage-plugin-orchestrator-form-widgets-1.8.0.tgz
npm publish backstage-plugin-scaffolder-backend-module-orchestrator-dynamic-1.8.0.tgz
```

6. Apply the redhat-developer-hub-orchestrator-infra Helm chart and approve the install plans. See Air-gapped installation with Helm chart instructions for details.
7. Apply the RHDH 1.8 Helm chart. Include the version 1.8.0 and enable the Orchestrator plugin, as shown in the following example:

```yaml
orchestrator.enabled=true
```

8. The RHDH 1.8 Helm chart defaults to pulling Orchestrator plugins from the official Red Hat NPM registry using full URL references. You must override this behavior to point to your local registry.

To configure the Orchestrator plugins to use a custom registry, complete the following steps:
* Open your values.yaml file.
* Explicitly list the Orchestrator plugin packages under the orchestrator.plugins section.

You must replace the simplified package references with the full URLs that point to your custom NPM registry, as shown in the following example:

```yaml
orchestrator:
  plugins:
    - disabled: false
      package: <custom_NPM_registry_URL>[:<port>]/@redhat/backstage-plugin-orchestrator-backend-dynamic/-/backstage-plugin-orchestrator-backend-dynamic-{product-bundle-version}.tgz
      integrity: sha512-xxxxxx
    - disabled: false
      package: <custom_NPM_registry_URL>[:<port>]/@redhat/backstage-plugin-orchestrator/-/backstage-plugin-orchestrator-{product-bundle-version}.tgz
      integrity: sha512-xxxxxy
    - disabled: false
      package: <custom_NPM_registry_URL>[:<port>]/@redhat/backstage-plugin-orchestrator-form-widgets/-/backstage-plugin-orchestrator-form-widgets-{product-bundle-version}.tgz
      integrity: sha512-xxxxxz
    - disabled: false
      package: <custom_NPM_registry_URL>[:<port>]/@redhat/backstage-plugin-scaffolder-backend-module-orchestrator-dynamic/-/backstage-plugin-scaffolder-backend-module-orchestrator-dynamic-{product-bundle-version}.tgz
      integrity: sha512-xxxx1
```


where:
<custom_NPM_registry_URL>:: Enter the address of your custom registry and make sure the integrity checksum, such as sha512-xxxxxx, matches the files in your registry.

* Restart the RHDH pod and wait for the components to deploy properly.
* After deployment is complete, go to the RHDH UI and confirm that the Orchestrator UI is accessible and functioning correctly.


[NOTE]
----
The successful accessibility of the Orchestrator UI confirms that the underlying components are running and the cluster recognizes the plugin.
----

# Installing Orchestrator plugin in an air-gapped environment with the Operator

You can configure Red Hat Developer Hub (RHDH) with the Orchestrator plugin in a fully disconnected or partially disconnected environment by using the Operator.

## Installing Red Hat Developer Hub with Orchestrator in a fully disconnected OpenShift Container Platform environment using the Operator

You can install Red Hat Developer Hub with Orchestrator plugin in a fully air-gapped environment using the Operator.

A disconnected installation prevents unauthorized access, data transfer, or communication with external sources.

You can use the helper script to install Red Hat Developer Hub by mirroring the Operator-related images to disk and transferring them to your disconnected environment without any connection to the internet.

* You have mirrored the Red Hat Developer Hub Operator images to the local registry using the RHDH mirroring script. For more information, see Installing Red Hat Developer Hub in a fully disconnected environment with the Operator.
* You have set up your disconnected environment using a local registry.
* You have permissions to push NPM packages to an NPM server available in your restricted network.
* You have installed the oc-mirror tool, with a version corresponding to the version of your OpenShift Container Platform cluster.

1. Create an ImageSetConfiguration file for oc-mirror. You must include the images and operators required by the Serverless Logic Operator in the ImageSetConfiguration file, as oc-mirror does not automatically mirror all images. Use the following example:

```yaml
apiVersion: mirror.openshift.io/v2alpha1
kind: ImageSetConfiguration
mirror:
  additionalimages:
  - name: registry.redhat.io/openshift-serverless-1/logic-jobs-service-postgresql-rhel8:1.36.0
  - name: registry.redhat.io/openshift-serverless-1/logic-jobs-service-ephemeral-rhel8:1.36.0
  - name: registry.redhat.io/openshift-serverless-1/logic-data-index-postgresql-rhel8:1.36.0
  - name: registry.redhat.io/openshift-serverless-1/logic-data-index-ephemeral-rhel8:1.36.0
  - name: registry.redhat.io/openshift-serverless-1/logic-db-migrator-tool-rhel8:1.36.0
  - name: registry.redhat.io/openshift-serverless-1/logic-swf-builder-rhel8:1.36.0
  - name: registry.redhat.io/openshift-serverless-1/logic-swf-devmode-rhel8:1.36.0
  operators:
    - catalog: registry.redhat.io/redhat/redhat-operator-index:4.19
      # For example: registry.redhat.io/redhat/redhat-operator-index:v4.19
      packages:
      - name: logic-operator-rhel8
        channels:
        - name: alpha
          minVersion: 1.36.0
          maxVersion: 1.36.0
      - name: serverless-operator
        channels:
        - name: stable
          minVersion: 1.36.0
          maxVersion: 1.36.1
```


Alternatively, you can use podman commands to find the missing images and add them to the additionalimages list if the versions change:

```terminal
IMG=registry.redhat.io/openshift-serverless-1/logic-operator-bundle:1.36
mkdir local-manifests-osl
podman create --name temp-container "$IMG" -c "cat /manifests/logic-operator-rhel8.clusterserviceversion.yaml"
podman cp temp-container:/manifests ./local-manifests-osl
podman rm temp-container
yq -r '.data."controllers_cfg.yaml" | from_yaml | .. | select(tag == "!!str") | select(test("^.\\/.:.*$"))' ./local-manifests-osl/manifests/logic-operator-rhel8-controllers-config_v1_configmap.yaml
```

2. Mirror the images in the ImageSetConfiguration.yaml file by running the oc-mirror command. For example:

```terminal
oc-mirror --config=ImageSetConfiguration.yaml file:///path/to/mirror-archive --authfile /path/to/authfile  --v2
```


[NOTE]
----
The oc-mirror command generates a local workspace containing the mirror archive files and the required cluster manifests.
----
3. Transfer the directory specified by /path/to/mirror-archive to a bastion host within your disconnected environment.
4. From the bastion host which has access to the mirror registry, mirror the images from the disk directory to your target registry. For example:

```yaml
oc-mirror --v2 --from <mirror-archive-file> docker://<target-registry-url:port> --workspace file://<workspace folder> --authfile /path/to/authfile
```


where:
<mirror-archive-file>:: Enter the name of the transferred tar file.
<target-registry-url:port>:: Enter your local registry, for example, registry.localhost:5000.
5. Apply the cluster-wide resources generated during the push step to redirect all image pulls to your local registry, as shown in the following example:

```terminal
cd <workspace folder>/working-dir/cluster-resources/
oc apply -f .
```

6. Download the Node Package Manager (NPM) packages for orchestrator 1.8.0 using any of the following methods:
* Download them as tgz files from the following registry:
* https://npm.registry.redhat.com/@redhat/backstage-plugin-orchestrator/-/backstage-plugin-orchestrator-1.8.0.tgz
* https://npm.registry.redhat.com/@redhat/backstage-plugin-orchestrator-backend-dynamic/-/backstage-plugin-orchestrator-backend-dynamic-1.8.0.tgz
* https://npm.registry.redhat.com/@redhat/backstage-plugin-scaffolder-backend-module-orchestrator-dynamic/-/backstage-plugin-scaffolder-backend-module-orchestrator-dynamic-1.8.0.tgz
* https://npm.registry.redhat.com/@redhat/backstage-plugin-orchestrator-form-widgets/-/backstage-plugin-orchestrator-form-widgets-1.8.0.tgz
* Alternatively, use the NPM packages from Red Hat NPM registry as shown in the following example:

```
npm pack "@redhat/backstage-plugin-orchestrator@1.8.0" --registry=https://npm.registry.redhat.com
npm pack "@redhat/backstage-plugin-orchestrator-backend-dynamic@1.8.0" --registry=https://npm.registry.redhat.com
npm pack "@redhat/backstage-plugin-scaffolder-backend-module-orchestrator-dynamic@1.8.0 --registry=https://npm.registry.redhat.com
npm pack "@redhat/backstage-plugin-orchestrator-form-widgets@1.8.0" --registry=https://npm.registry.redhat.com
```

7. Push the NPM packages you have downloaded to your NPM server, as shown in the following example:

```
npm publish backstage-plugin-orchestrator-1.8.0.tgz
npm publish backstage-plugin-orchestrator-backend-dynamic-1.8.0.tgz
npm publish backstage-plugin-orchestrator-form-widgets-1.8.0.tgz
npm publish backstage-plugin-scaffolder-backend-module-orchestrator-dynamic-1.8.0.tgz
```

8. Install the OpenShift Serverless Operator and OpenShift Serverless Logic Operators using OperatorHub.
9. Create a Backstage custom resource (CR).
10. Configure the Backstage CR for the Orchestrator as described in the Orchestrator plugin dependencies for Operator installation.

Create all the resources and configure the Backstage instance accordingly. See Configuring a custom NPM registry for instructions on how to point RHDH towards the custom NPM registry.

* Restart the RHDH pod and wait for the components to deploy properly.
* Once stable, go to the RHDH UI, and confirm that the Orchestrator UI is accessible and functioning correctly.


[NOTE]
----
The successful accessibility of the Orchestrator UI confirms that the underlying components are running and the cluster recognizes the plugin.
----

## Installing Red Hat Developer Hub with Orchestrator in a partially disconnected OpenShift Container Platform environment using the Operator

You can install Red Hat Developer Hub with Orchestrator plugin in a partial air-gapped environment using the Operator.

A disconnected installation prevents unauthorized access, data transfer, or communication with external sources.

You can use the oc-mirror command to mirror resources directly to your accessible local mirror registry and apply the generated cluster resources.

* You have mirrored the Red Hat Developer Hub Operator images to the local registry using the RHDH mirroring script. For more information, see Installing Red Hat Developer Hub in a partially disconnected environment with the Operator.
* You have set up your disconnected environment using a local registry.
* You have permissions to push NPM packages to an NPM server available in your restricted network.
* You have installed the oc-mirror tool, with a version corresponding to the version of your OpenShift Container Platform cluster.

1. Create an ImageSetConfiguration file for oc-mirror. You must include the images and operators required by the Serverless Logic Operator in the ImageSetConfiguration file, as oc-mirror does not automatically mirror all images. Use the following example:

```yaml
apiVersion: mirror.openshift.io/v2alpha1
kind: ImageSetConfiguration
mirror:
  additionalimages:
  - name: registry.redhat.io/openshift-serverless-1/logic-jobs-service-postgresql-rhel8:1.36.0
  - name: registry.redhat.io/openshift-serverless-1/logic-jobs-service-ephemeral-rhel8:1.36.0
  - name: registry.redhat.io/openshift-serverless-1/logic-data-index-postgresql-rhel8:1.36.0
  - name: registry.redhat.io/openshift-serverless-1/logic-data-index-ephemeral-rhel8:1.36.0
  - name: registry.redhat.io/openshift-serverless-1/logic-db-migrator-tool-rhel8:1.36.0
  - name: registry.redhat.io/openshift-serverless-1/logic-swf-builder-rhel8:1.36.0
  - name: registry.redhat.io/openshift-serverless-1/logic-swf-devmode-rhel8:1.36.0
  operators:
    - catalog: registry.redhat.io/redhat/redhat-operator-index:4.19
      # For example: registry.redhat.io/redhat/redhat-operator-index:v4.19
      packages:
      - name: logic-operator-rhel8
        channels:
        - name: alpha
          minVersion: 1.36.0
          maxVersion: 1.36.0
      - name: serverless-operator
        channels:
        - name: stable
          minVersion: 1.36.0
          maxVersion: 1.36.1
```


Alternatively, you can use the podman commands to find the missing images and add them to the additionalimages list if the versions change:

```terminal
IMG=registry.redhat.io/openshift-serverless-1/logic-operator-bundle:1.36.0-8
mkdir local-manifests-osl
podman create --name temp-container "$IMG" -c "cat /manifests/logic-operator-rhel8.clusterserviceversion.yaml"
podman cp temp-container:/manifests ./local-manifests-osl
podman rm temp-container
yq -r '.data."controllers_cfg.yaml" | from_yaml | .. | select(tag == "!!str") | select(test("^.\\/.:.*$"))' ./local-manifests-osl/manifests/logic-operator-rhel8-controllers-config_v1_configmap.yaml
```

2. Mirror the images in the ImageSetConfiguration.yaml file by running the oc-mirror command. For example:

```terminal
oc-mirror --config=imagesetconfiguration.yaml docker://<registry URL:port> --workspace file://<workspace folder> --authfile /path/to/authfile  --v2
cd <workspace folder>/working-dir/cluster-resources/
oc apply -f .
```

3. Download the Node Package Manager (NPM) packages for orchestrator 1.8.0 using any of the following methods:
* Download them as tgz files from the following registry:
* https://npm.registry.redhat.com/@redhat/backstage-plugin-orchestrator/-/backstage-plugin-orchestrator-1.8.0.tgz
* https://npm.registry.redhat.com/@redhat/backstage-plugin-orchestrator-backend-dynamic/-/backstage-plugin-orchestrator-backend-dynamic-1.8.0.tgz
* https://npm.registry.redhat.com/@redhat/backstage-plugin-scaffolder-backend-module-orchestrator-dynamic/-/backstage-plugin-scaffolder-backend-module-orchestrator-dynamic-1.8.0.tgz
* https://npm.registry.redhat.com/@redhat/backstage-plugin-orchestrator-form-widgets/-/backstage-plugin-orchestrator-form-widgets-1.8.0.tgz
* Alternatively, use the NPM packages from Red Hat NPM registry as shown in the following example:

```
npm pack "@redhat/backstage-plugin-orchestrator@1.8.0" --registry=https://npm.registry.redhat.com
npm pack "@redhat/backstage-plugin-orchestrator-backend-dynamic@1.8.0" --registry=https://npm.registry.redhat.com
npm pack "@redhat/backstage-plugin-scaffolder-backend-module-orchestrator-dynamic@1.8.0 --registry=https://npm.registry.redhat.com
npm pack "@redhat/backstage-plugin-orchestrator-form-widgets@1.8.0" --registry=https://npm.registry.redhat.com
```

4. Push the NPM packages you have downloaded to your NPM server, as shown in the following example:

```
npm publish backstage-plugin-orchestrator-1.8.0.tgz
npm publish backstage-plugin-orchestrator-backend-dynamic-1.8.0.tgz
npm publish backstage-plugin-orchestrator-form-widgets-1.8.0.tgz
npm publish backstage-plugin-scaffolder-backend-module-orchestrator-dynamic-1.8.0.tgz
```

5. Install the OpenShift Serverless Operator and OpenShift Serverless Logic Operators using OperatorHub.
6. Create a Backstage custom resource (CR).
7. Configure the Backstage CR for the Orchestrator as described in the Orchestrator plugin dependencies for Operator installation.

Create all the resources and configure the Backstage instance accordingly. See Configuring a custom NPM registry for instructions on how to point RHDH towards the custom NPM registry.

* Restart the RHDH pod and wait for the components to deploy properly.
* Once stable, go to the RHDH UI, and confirm that the Orchestrator UI is accessible and functioning correctly.


[NOTE]
----
The successful accessibility of the Orchestrator UI confirms that the underlying components are running and the cluster recognizes the plugin.
----